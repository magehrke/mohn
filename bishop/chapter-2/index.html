

<!doctype html>
<html class="no-js" lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Chapter 2 - MOHN</title>
  <meta name="description" content="Collection of Machine Learning Resources">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" type="image/png" href="/mohn/img/favicon.png"/>
  <link rel="stylesheet" href="https://yui-s.yahooapis.com/pure/0.6.0/pure-min.css">
  <!--[if lte IE 8]>
  <link rel="stylesheet" href="http://yui.yahooapis.com/pure/0.6.0/grids-responsive-old-ie-min.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="https://yui-s.yahooapis.com/pure/0.6.0/grids-responsive-min.css">
  <!--<![endif]-->
  <link rel="stylesheet" href="/mohn/css/main.min.css">

  <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       tex2jax: {
         skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
         inlineMath: [['$','$']]
       }
     });
   </script>
   <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

</head>
<body>






<header class="site-header">
  <div class="container">
      <h1 class="logo">
        <a href="/mohn/"></a>
      </h1>
      <nav class="main-nav">
        <ul class="navigation">
          
          <li class="">
            
            <a href="/mohn/">
                  Home
                  
                </a>
          </li>
          
          <li class="">
            
            <a href="/mohn/paperview/">
                  Paperview
                  
                </a>
          </li>
          
          <li class="">
            
            <a href="/mohn/paper/">
                  Paper
                  
                </a>
          </li>
          
          <li class="">
            
            <a href="/mohn/bishop/">
                  Bishop
                  
                </a>
          </li>
          
          <li class="">
            
            <a href="/mohn/projects/">
                  Projects
                  
                </a>
          </li>
          
        </ul>
    </nav>
  </div>
</header>



	<header id="banner" style="background-image: url('/mohn/uploads/books.jpg');">
		<div class="container pure-g">
			<div class="pure-u-1 pure-u-md-1">
				<header class="call-to-action">
					<h1>Chapter 2</h1>
					
						<h2>Probability Distributions</h2>
					
					
				</div>
			</header>
		</div>
	</header>
	









<nav class="breadcrumbs">
	<div class="container">
		<div class="pure-u-1">
        <div class="content content-narrow">
					
						<ul>
							<li><a href="/mohn">Home</a>&nbsp;<i class="fa fa-chevron-right"></i></li>
							
								
									
									
									
									<li>
										<a href="/mohn/bishop/">Bishop</a>&nbsp;<i class="fa fa-chevron-right"></i>
									</li>
								
							
								
									<li class="active">Chapter 2</li>
								
							
						</ul>
        </div>
    </div>
	</div>
</nav>


<div class="container pure-g">
    <div class="pure-u-1">
        <div class="content content-narrow">
            <h4 id="density-estimation">Density Estimation</h4>
<h4 id="parametric-distributions">Parametric Distributions</h4>
<ul>
  <li>Examples: Gaussian, Multinomial Distribution</li>
</ul>

<h4 id="conjugate-priors">Conjugate Priors</h4>
<h4 id="nonparametric-distributions">Nonparametric Distributions</h4>
<ul>
  <li>Examples: Histograms, Nearest-Neighbours, Kernels</li>
</ul>

<h2 id="21-binary-variables">2.1 Binary Variables</h2>
<h4 id="bernoulli-distribution">Bernoulli distribution:</h4>
<ul>
  <li>Probability for a binary variable.</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$\text{Bern}(x</td>
          <td>\mu) = \mu^x(1-\mu)^{1-x}$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Normalized</li>
  <li>Mean: $\text{E}[x] = \mu$</li>
  <li>Variance: $\text{var}[x] = \mu(1-\mu)$</li>
  <li>Likelihood: $p(D|\mu) = \prod_{n=1}^{N}p(x_n|\mu)
= \prod_{n=1}^{N}\mu^{x_n} (1-\mu)^{1-x_n}$</li>
  <li>log Likelihood: $\ln p(D|\mu) = \sum_{n=1}^{N}p(x_n|\mu)
= \sum_{n=1}^{N} x_n  \ln\mu + (1-x_n) \ln (1-\mu)$</li>
  <li>ML solution: sample mean, which can be written as $\mu_{ML} = \dfrac{1}{N}$ if
$m$ is the amount of $x = 1$.</li>
</ul>

<h4 id="sample-mean">Sample Mean:</h4>
<ul>
  <li>$\mu = \dfrac{1}{N}\sum_{n=1}^N x_n$</li>
</ul>

<h4 id="binomial-distribution">Binomial distribution:</h4>
<ul>
  <li>Probability that a binary variable is $m$ times $x = 1$.</li>
  <li>Binomial distribution is proportional to Binomial Likelihood.</li>
  <li>$\text{Bin}(m|N, \mu) = \binom{N}{m} \mu^m (1 - \mu)^{N-m} ~ \text{where} ~
\binom{N}{m} \equiv \dfrac{N!}{(N-m)!m!}$</li>
</ul>

<h3 id="211-the-beta-distribution">2.1.1 The beta distribution</h3>

<h2 id="25-nonparametric-methods">2.5 Nonparametric Methods</h2>

<h4 id="introduction">Introduction:</h4>
<ul>
  <li>Nonparametric approaches to density estimation make few assumptions about the
form of the distribution.</li>
  <li>The following methods are frequentist methods, however nonparametric Bayesian
methods attract increasing interest.</li>
</ul>

<h4 id="histograms">Histograms:</h4>
<ul>
  <li>Single continuous variable</li>
  <li>Partition x into distinct bins of with $\Delta_i$</li>
  <li>Count the number $n_i$ of observations of $x$ falling in bin $i$.</li>
  <li>Probability: $p_i = \dfrac{n_i}{N\Delta_i}$</li>
  <li>Most of the time all $\Delta_i$ have the same width ($\Delta_i = \Delta$).</li>
  <li>If $\Delta$ is very small, the resulting density model is very spiky with
lots of structure which is not present in the underlying distribution.</li>
  <li>If $\Delta$ is too large, the result is too smooth.</li>
  <li>In principle, histograms depend on the choice of edge location, though this
is less significant than the value of $\Delta$.</li>
  <li>Once the histogram has been computed, the data can be discarded (unlike the
other two methods)
    <ul>
      <li>Good if data set is too large or the data arrives sequentially.</li>
    </ul>
  </li>
  <li>In practice, histograms are used for quick visualizations in 1D or 2D.
    <ul>
      <li>Unsuited to most density estimation applications.</li>
      <li>Discontinuities at bin edges</li>
      <li>Curse of dimensionality: If we divide each variable in a D-dimensional
space into M bins, the total number of bins will be $M^D$.</li>
    </ul>
  </li>
</ul>

<h4 id="lessons-from-histograms">Lessons from histograms:</h4>

<ul>
  <li>To estimate probability densities at a specific location, we should consider
the data points that lie within some local neighbourhood of that.
    <ul>
      <li>Concept of locality needs some form of distance measure (e.g. Euclidean).</li>
      <li>Spacial extend of the local region is a natural smoothing parameter.</li>
    </ul>
  </li>
  <li>The value of the smoothing parameter should be neither too small or too large.</li>
</ul>

<h3 id="251-kernel-density-estimators">2.5.1 Kernel Density Estimators</h3>

<h4 id="assumptions">Assumptions:</h4>
<ul>
  <li>Unknown density p(x) in some D-dimensional space</li>
  <li>Euclidean distance</li>
  <li>region R containing x</li>
  <li>N collected observations</li>
</ul>

<h4 id="probability-mass">Probability mass:</h4>
<ul>
  <li>a function that gives the probability that a discrete random variable is
exactly equal to some value.</li>
  <li>
    <p>It differs from a probability density function
in that the latter is associated with continuous rather than discrete random
variables; the values of the probability density function are not probabilities
as such: a PDF must be integrated over an interval to yield a probability.</p>
  </li>
  <li>
    <p>Probability mass associated with region $R$ is given by
$P = \int_R p(\textbf{x}) \text{d}\textbf{x}$</p>
  </li>
  <li>
    <p>Because each data point has probability $P$ of falling within $R$, the total
number $K$ of points that lie inside $R$ will be distributed according to the
binomial distribution:
$\text{Bin}(K|N, P) = \dfrac{N!}{(N-K)!K!} P^K (1 - P)^{N-K}$</p>
  </li>
  <li>Mean Fraction of points that fall into the region: $E[K/N] = P$</li>
  <li>Variance: $\text{var}[K/N] = P(1-P)/N$</li>
</ul>

<h4 id="density-estimate">Density estimate:</h4>
<ul>
  <li>For large N this distribution with be sharply peaked around the mean and so
[K \approx NP]</li>
  <li>If the region $R$ is sufficiently small that the probability density $p(x)$
is roughly constant over the region, then we have
[P \approx p(\textbf{x})V]</li>
  <li>
    <p>With this we obtain our density estimate:
[p(\textbf{x}) = \dfrac{K}{NV}]</p>
  </li>
  <li>This depends on two assumptions: (a) the region $R$ is sufficiently small that
the density is appoximately constant over the region and (b) sufficiently large
that the number $K$ of points falling inside the region is sufficient for the
binomial distribution to be sharply peaked.</li>
</ul>

<h4 id="2-ways-to-estimate-the-density">2 ways to estimate the density:</h4>
<ul>
  <li>Fix $K$ and determine $V$ from the data = $K$-nearest-neighbours</li>
  <li>Fix $V$ and determine $K$ from the data = kernel density estimator</li>
  <li>Both converge to the true probability, if $V$ shrinks with N and $K$ grows
with N</li>
</ul>

<h4 id="kernel-density-estimator">Kernel density estimator:</h4>
<ul>
  <li>
    <p>Simple kernel function (here also called Parzen window) with a unit cube
centered around the origin:
[k(\textbf{u}) =   <br />
\begin{cases}
    1, &amp; \text{if}\ |u_i| \le 1/2, i = 1, \dots , D <br />
    0, &amp; \text{otherwise}
\end{cases}]</p>
  </li>
  <li>
    <p>The total number of data points lying insida a cube of side $h$ is:
[K = \sum_{n=1}^N k \left(\dfrac{\textbf{x} - \textbf{x}_n}{h}\right)]</p>
  </li>
  <li>
    <p>Now, a kernel density estimator (also called Parzen estimator) is the class of
density models given by:
[p(\textbf{x}) = \dfrac{1}{N} \sum_{n=1}^N \dfrac{1}{h^D} k
\left(\dfrac{\textbf{x} - \textbf{x}_n}{h}\right)]</p>
  </li>
  <li>We have used $V = h^D$</li>
  <li>No computation involved in the training phase, this simply requires storage!</li>
  <li>However, this is also one of its great weaknesses; the computational cost of
evaluating the density grows linearly with the size of the data set.</li>
  <li>This estimator will suffer from discontinuities at the boundaries of the cubes</li>
</ul>

<h4 id="guassian-kernel-density-estimator">Guassian kernel density estimator:</h4>
<ul>
  <li>We can obtain a smoother kernel function, if we chose a Gaussian kernel</li>
</ul>

<p>[p(\textbf{x}) = \dfrac{1}{N} \sum_{n=1}^N \dfrac{1}{(2\pi h^2)^{D/2}} \exp
\left{\dfrac{||\textbf{x} - \textbf{x}_n||^2}{2h^2}\right}]</p>

<ul>
  <li>where $h$ represents the standard deviation of the Gaussian component.</li>
  <li>Like with histograms, our hyperparameter ($h$ in this case) is a smoothing
parameter.</li>
  <li>The optimization of $h$ is a problem of model complexity analogous to the
choice of bin width or degree of polynomial.</li>
</ul>

<h4 id="choice-of-kernels">Choice of Kernels:</h4>
<ul>
  <li>We can choose any kernel function $k(\mathbf{u})$ that satisfies
$k(\mathbf{u}) \ge 0$ and $\int k(\mathbf{u}) \text{d}\textbf{u} = 1$.</li>
  <li>This ensures that the resulting probability dist is nonnegative and integrates
to one.</li>
</ul>

<h3 id="252-nearest-neighbour-methods">2.5.2 Nearest-neighbour methods</h3>

<h4 id="k-nearest-neighbour">K-nearest-neighbour:</h4>
<ul>
  <li>One of the difficulties of kernel methods is that the parameter $h$ is fixed
for all kernels / data points. Depending on how many points lie in a region,
$h$ might be perfectly chosen for some location and poorly for others.</li>
  <li>We can fix this in NN by choosing a fixed $K$ and determining $V$.</li>
  <li>We consider a small sphere centered on the point x and allow the radius to
grow until it contains precisely $K$ data points.</li>
  <li>The estimate is then given by $p(\textbf{x}) = {K}/{NV}$ with $V$ set to the
volume of the resulting sphere.</li>
  <li>Now the parameter $K$ governs the degree of smoothing</li>
  <li>Again, K should neither be too large nor too small.</li>
  <li>The model produced by K-NN is not a true density model.</li>
</ul>

<h4 id="k-nn-for-classification">K-NN for classification:</h4>
<ul>
  <li>Apply K-NN seperately for each class and make use of Bayes’ theorem.</li>
  <li>Assume a data set with $N_k$ points in class $C_k$ with N points in total</li>
  <li>If we with to classify a new point x, we draw a sphere around x  containing
precisely K points irrespective their class.</li>
  <li>Suppose this squere has volume $V$ and contains $K_k$ points from class $C_k$.
&lt;/br&gt;&lt;/br&gt;</li>
  <li>The density associated with each class:
[p(\textbf{x}|C_k) = \dfrac{K_k}{N_kV}]</li>
  <li>The unconditional density:
[p(\textbf{x}) = \dfrac{K}{NV}]</li>
  <li>The class priors:
[p(C_k) = \dfrac{N_k}{N}]</li>
  <li>
    <p>The posterior:
[p(C_k|\textbf{x}) = \dfrac{p(\textbf{x}|C_k)p(C_k)}
{p(\textbf{x})} = \dfrac{K_k}{K}]</p>
  </li>
  <li>Minimizing the misclassification probability is done by assigning x to the
class having the largest posterior probability.</li>
</ul>

<h4 id="the-nearest-neighbour-rule">The nearest neighbour rule:</h4>
<ul>
  <li>K = 1</li>
  <li>An unseen point is simply assigned to the nearest point from the training set.</li>
  <li>In the limit $N \rightarrow \infty$ the error rate is never more than twice
the minimum achievable error rate of an optimal classifier.</li>
</ul>

<h4 id="sum-up-nonparametric-models">Sum up nonparametric models:</h4>
<ul>
  <li>Both K-NN and kernel density estimator need the entire training data set to be
stored, leading to expensive computation if the data set is large.</li>
  <li>Tree-based search structures allow finding neighbours efficiently</li>
  <li>Nevertheless, nonparametric methods are still severly limited</li>
  <li>
    <p>However, we have seen that simple parametric models are very restricted in
terms of the forms of distribution that they can represent</p>
  </li>
  <li>We need to find density models that are very flexible and yet for which the
complexity of the models can be  controlled independently of the training
set size</li>
</ul>

        </div>
    </div>
</div>


<footer class="site-footer">
	<div class="container pure-g">
        <div class="footer-col pure-u-1 pure-u-md-1-4">
            <img src="/mohn/img/logo-white.png" alt="" class="footer-logo">
        </div>
        <!-- <div class="footer-col pure-u-1 pure-u-md-1-4">
            <h3>About MOHN</h3>
            <ul>
            
            </ul>
        </div>
        <div class="footer-col pure-u-1 pure-u-md-1-4">
            <h3>Projects</h3>
            <ul>
            
                <li><a href="/mohn/projects/activation-functions/">Activation Functions</a></li>
            
                <li><a href="/mohn/projects/optimization-algorithms/">Gradient Descent Optimization Algorithms</a></li>
            
                <li><a href="/mohn/projects/terms/">Terms</a></li>
            
            
                <li><a href="/mohn/projects/">See all projects</a></li>
            
            </ul>
        </div> -->
        <div class="footer-col pure-u-1 pure-u-md-1-4">
            <h3>Contact</h3>
            



<p>
  MOHN<br />
  <br />
  <!--  -->
  <!--  -->
  <!--  -->
  <!-- <br /> -->
  <!--  -->
  <a href="mailto:max.alex@online.de" title="max.alex@online.de">max.alex@online.de</a>
</p>


        </div>
	</div>
	<div class="copyright">
        <p>
            &copy;2019 MOHN - All Rights Reserved<br>
			<a href="/mohn/admin">Site Admin</a>
		</p>
	</div>
</footer>
<!-- Javascript -->
<script src="/mohn/js/main.js" type="text/javascript"></script>
</body>
</html>


